{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cqCt_GhvCnwY"
   },
   "source": [
    "# VQ-VAE training example\n",
    "\n",
    "Demonstration of how to train the model specified in https://arxiv.org/abs/1711.00937, using TF 2 / Sonnet 2.\n",
    "\n",
    "On Mac and Linux, simply execute each cell in turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "95YuC82P35Of"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Uncomment the line below if running on colab.research.google.com\n",
    "# !pip install \"dm-sonnet>=2.0.0b0\" --pre\n",
    "\n",
    "try:\n",
    "  # internal\n",
    "  import tensorflow.compat.v2 as tf\n",
    "  import sonnet.v2 as snt\n",
    "except ImportError:\n",
    "  # public\n",
    "  import tensorflow as tf\n",
    "  import sonnet as snt\n",
    "\n",
    "import tarfile\n",
    "\n",
    "from six.moves import cPickle\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange\n",
    "\n",
    "# Uncomment the line below if tensorflow version < 2.0\n",
    "# tf.enable_v2_behavior()\n",
    "\n",
    "print(\"TensorFlow version {}\".format(tf.__version__))\n",
    "print(\"Sonnet version {}\".format(snt.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DT8fKmqQC35h"
   },
   "source": [
    "# Download Cifar10 data\n",
    "This requires a connection to the internet and will download ~160MB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mR0lkHXDC3Pz",
    "outputId": "5119d8a3-5556-4fd3-c9d1-0caa71065687"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted data files to /tmp/tmp59qfagq0\n"
     ]
    }
   ],
   "source": [
    "data_path = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "\n",
    "local_data_dir = tempfile.mkdtemp()  # Change this as needed\n",
    "os.makedirs(local_data_dir, exist_ok=True)\n",
    "\n",
    "url = urllib.request.urlopen(data_path)\n",
    "archive = tarfile.open(fileobj=url, mode='r|gz') # read a .tar.gz stream\n",
    "archive.extractall(local_data_dir)\n",
    "url.close()\n",
    "archive.close()\n",
    "print('extracted data files to %s' % local_data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lUgvEhfJyQLZ"
   },
   "source": [
    "# Load the data into Numpy\n",
    "We compute the variance of the whole training set to normalise the Mean Squared Error below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9C-V2D6RSQwl"
   },
   "outputs": [],
   "source": [
    "def unpickle(filename):\n",
    "  with open(filename, 'rb') as fo:\n",
    "    return cPickle.load(fo, encoding='latin1')\n",
    "  \n",
    "def reshape_flattened_image_batch(flat_image_batch):\n",
    "  return flat_image_batch.reshape(-1, 3, 32, 32).transpose([0, 2, 3, 1])  # convert from NCHW to NHWC\n",
    "\n",
    "def combine_batches(batch_list):\n",
    "  images = np.vstack([reshape_flattened_image_batch(batch['data'])\n",
    "                      for batch in batch_list])\n",
    "  labels = np.vstack([np.array(batch['labels']) for batch in batch_list]).reshape(-1, 1)\n",
    "  return {'images': images, 'labels': labels}\n",
    "  \n",
    "\n",
    "train_data_dict = combine_batches([\n",
    "    unpickle(os.path.join(local_data_dir,\n",
    "                          'cifar-10-batches-py/data_batch_%d' % i))\n",
    "    for i in range(1,5)\n",
    "])\n",
    "\n",
    "valid_data_dict = combine_batches([\n",
    "    unpickle(os.path.join(local_data_dir,\n",
    "                          'cifar-10-batches-py/data_batch_5'))])\n",
    "\n",
    "test_data_dict = combine_batches([\n",
    "    unpickle(os.path.join(local_data_dir, 'cifar-10-batches-py/test_batch'))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cIRl2ZtxoKNz",
    "outputId": "e1df9efa-b5b3-4f82-809e-80cac2581bce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data variance: 0.06336906706339507\n"
     ]
    }
   ],
   "source": [
    "def cast_and_normalise_images(data_dict):\n",
    "  \"\"\"Convert images to floating point with the range [-0.5, 0.5]\"\"\"\n",
    "  images = data_dict['images']\n",
    "  data_dict['images'] = (tf.cast(images, tf.float32) / 255.0) - 0.5\n",
    "  return data_dict\n",
    "\n",
    "train_data_variance = np.var(train_data_dict['images'] / 255.0)\n",
    "print('train data variance: %s' % train_data_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jse__pEBAkvI"
   },
   "source": [
    "# Encoder & Decoder Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1gwD36Vr6KqA"
   },
   "outputs": [],
   "source": [
    "class ResidualStack(snt.Module):\n",
    "  def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens,\n",
    "               name=None):\n",
    "    super(ResidualStack, self).__init__(name=name)\n",
    "    self._num_hiddens = num_hiddens\n",
    "    self._num_residual_layers = num_residual_layers\n",
    "    self._num_residual_hiddens = num_residual_hiddens\n",
    "\n",
    "    self._layers = []\n",
    "    for i in range(num_residual_layers):\n",
    "      conv3 = snt.Conv2D(\n",
    "          output_channels=num_residual_hiddens,\n",
    "          kernel_shape=(3, 3),\n",
    "          stride=(1, 1),\n",
    "          name=\"res3x3_%d\" % i)\n",
    "      conv1 = snt.Conv2D(\n",
    "          output_channels=num_hiddens,\n",
    "          kernel_shape=(1, 1),\n",
    "          stride=(1, 1),\n",
    "          name=\"res1x1_%d\" % i)\n",
    "      self._layers.append((conv3, conv1))\n",
    "\n",
    "  def __call__(self, inputs):\n",
    "    h = inputs\n",
    "    for conv3, conv1 in self._layers:\n",
    "      conv3_out = conv3(tf.nn.relu(h))\n",
    "      conv1_out = conv1(tf.nn.relu(conv3_out))\n",
    "      h += conv1_out\n",
    "    return tf.nn.relu(h)  # Resnet V1 style\n",
    "\n",
    "\n",
    "class Encoder(snt.Module):\n",
    "  def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens,\n",
    "               name=None):\n",
    "    super(Encoder, self).__init__(name=name)\n",
    "    self._num_hiddens = num_hiddens\n",
    "    self._num_residual_layers = num_residual_layers\n",
    "    self._num_residual_hiddens = num_residual_hiddens\n",
    "\n",
    "    self._enc_1 = snt.Conv2D(\n",
    "        output_channels=self._num_hiddens // 2,\n",
    "        kernel_shape=(4, 4),\n",
    "        stride=(2, 2),\n",
    "        name=\"enc_1\")\n",
    "    self._enc_2 = snt.Conv2D(\n",
    "        output_channels=self._num_hiddens,\n",
    "        kernel_shape=(4, 4),\n",
    "        stride=(2, 2),\n",
    "        name=\"enc_2\")\n",
    "    self._enc_3 = snt.Conv2D(\n",
    "        output_channels=self._num_hiddens,\n",
    "        kernel_shape=(3, 3),\n",
    "        stride=(1, 1),\n",
    "        name=\"enc_3\")\n",
    "    self._residual_stack = ResidualStack(\n",
    "        self._num_hiddens,\n",
    "        self._num_residual_layers,\n",
    "        self._num_residual_hiddens)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    h = tf.nn.relu(self._enc_1(x))\n",
    "    h = tf.nn.relu(self._enc_2(h))\n",
    "    h = tf.nn.relu(self._enc_3(h))\n",
    "    return self._residual_stack(h)\n",
    "\n",
    "\n",
    "class Decoder(snt.Module):\n",
    "  def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens,\n",
    "               name=None):\n",
    "    super(Decoder, self).__init__(name=name)\n",
    "    self._num_hiddens = num_hiddens\n",
    "    self._num_residual_layers = num_residual_layers\n",
    "    self._num_residual_hiddens = num_residual_hiddens\n",
    "\n",
    "    self._dec_1 = snt.Conv2D(\n",
    "        output_channels=self._num_hiddens,\n",
    "        kernel_shape=(3, 3),\n",
    "        stride=(1, 1),\n",
    "        name=\"dec_1\")\n",
    "    self._residual_stack = ResidualStack(\n",
    "        self._num_hiddens,\n",
    "        self._num_residual_layers,\n",
    "        self._num_residual_hiddens)\n",
    "    self._dec_2 = snt.Conv2DTranspose(\n",
    "        output_channels=self._num_hiddens // 2,\n",
    "        output_shape=None,\n",
    "        kernel_shape=(4, 4),\n",
    "        stride=(2, 2),\n",
    "        name=\"dec_2\")\n",
    "    self._dec_3 = snt.Conv2DTranspose(\n",
    "        output_channels=3,\n",
    "        output_shape=None,\n",
    "        kernel_shape=(4, 4),\n",
    "        stride=(2, 2),\n",
    "        name=\"dec_3\")\n",
    "    \n",
    "  def __call__(self, x):\n",
    "    h = self._dec_1(x)\n",
    "    h = self._residual_stack(h)\n",
    "    h = tf.nn.relu(self._dec_2(h))\n",
    "    x_recon = self._dec_3(h)\n",
    "    return x_recon\n",
    "    \n",
    "\n",
    "class VQVAEModel(snt.Module):\n",
    "  def __init__(self, encoder, decoder, vqvae, pre_vq_conv1, \n",
    "               data_variance, name=None):\n",
    "    super(VQVAEModel, self).__init__(name=name)\n",
    "    self._encoder = encoder\n",
    "    self._decoder = decoder\n",
    "    self._vqvae = vqvae\n",
    "    self._pre_vq_conv1 = pre_vq_conv1\n",
    "    self._data_variance = data_variance\n",
    "\n",
    "  def __call__(self, inputs, is_training):\n",
    "    z = self._pre_vq_conv1(self._encoder(inputs))\n",
    "    vq_output = self._vqvae(z, is_training=is_training)\n",
    "    x_recon = self._decoder(vq_output['quantize'])\n",
    "    recon_error = tf.reduce_mean((x_recon - inputs) ** 2) / self._data_variance\n",
    "    loss = recon_error + vq_output['loss']\n",
    "    return {\n",
    "        'z': z,\n",
    "        'x_recon': x_recon,\n",
    "        'loss': loss,\n",
    "        'recon_error': recon_error,\n",
    "        'vq_output': vq_output,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FF7WaOn-s7En"
   },
   "source": [
    "# Build Model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "owGEoOkO4ttk"
   },
   "outputs": [],
   "source": [
    "# Set hyper-parameters.\n",
    "batch_size = 32\n",
    "image_size = 32\n",
    "\n",
    "# 100k steps should take < 30 minutes on a modern (>= 2017) GPU.\n",
    "num_training_updates = 100000\n",
    "\n",
    "num_hiddens = 128\n",
    "num_residual_hiddens = 32\n",
    "num_residual_layers = 2\n",
    "# These hyper-parameters define the size of the model (number of parameters and layers).\n",
    "# The hyper-parameters in the paper were (For ImageNet):\n",
    "# batch_size = 128\n",
    "# image_size = 128\n",
    "# num_hiddens = 128\n",
    "# num_residual_hiddens = 32\n",
    "# num_residual_layers = 2\n",
    "\n",
    "# This value is not that important, usually 64 works.\n",
    "# This will not change the capacity in the information-bottleneck.\n",
    "embedding_dim = 64\n",
    "\n",
    "# The higher this value, the higher the capacity in the information bottleneck.\n",
    "num_embeddings = 512\n",
    "\n",
    "# commitment_cost should be set appropriately. It's often useful to try a couple\n",
    "# of values. It mostly depends on the scale of the reconstruction cost\n",
    "# (log p(x|z)). So if the reconstruction cost is 100x higher, the\n",
    "# commitment_cost should also be multiplied with the same amount.\n",
    "commitment_cost = 0.25\n",
    "\n",
    "# Use EMA updates for the codebook (instead of the Adam optimizer).\n",
    "# This typically converges faster, and makes the model less dependent on choice\n",
    "# of the optimizer. In the VQ-VAE paper EMA updates were not used (but was\n",
    "# developed afterwards). See Appendix of the paper for more details.\n",
    "vq_use_ema = False\n",
    "\n",
    "# This is only used for EMA updates.\n",
    "decay = 0.99\n",
    "\n",
    "learning_rate = 3e-4\n",
    "\n",
    "\n",
    "# # Data Loading.\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(train_data_dict)\n",
    "    .map(cast_and_normalise_images)\n",
    "    .shuffle(10000)\n",
    "    .repeat(-1)  # repeat indefinitely\n",
    "    .batch(batch_size, drop_remainder=True))\n",
    "valid_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(valid_data_dict)\n",
    "    .map(cast_and_normalise_images)\n",
    "    .repeat(1)  # 1 epoch\n",
    "    .batch(batch_size))\n",
    "\n",
    "# # Build modules.\n",
    "encoder = Encoder(num_hiddens, num_residual_layers, num_residual_hiddens)\n",
    "decoder = Decoder(num_hiddens, num_residual_layers, num_residual_hiddens)\n",
    "pre_vq_conv1 = snt.Conv2D(output_channels=embedding_dim,\n",
    "    kernel_shape=(1, 1),\n",
    "    stride=(1, 1),\n",
    "    name=\"to_vq\")\n",
    "\n",
    "if vq_use_ema:\n",
    "  vq_vae = snt.nets.VectorQuantizerEMA(\n",
    "      embedding_dim=embedding_dim,\n",
    "      num_embeddings=num_embeddings,\n",
    "      commitment_cost=commitment_cost,\n",
    "      decay=decay)\n",
    "else:\n",
    "  vq_vae = snt.nets.VectorQuantizer(\n",
    "      embedding_dim=embedding_dim,\n",
    "      num_embeddings=num_embeddings,\n",
    "      commitment_cost=commitment_cost)\n",
    "  \n",
    "model = VQVAEModel(encoder, decoder, vq_vae, pre_vq_conv1,\n",
    "                   data_variance=train_data_variance)\n",
    "\n",
    "optimizer = snt.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "@tf.function\n",
    "def train_step(data):\n",
    "  with tf.GradientTape() as tape:\n",
    "    model_output = model(data['images'], is_training=True)\n",
    "  trainable_variables = model.trainable_variables\n",
    "  grads = tape.gradient(model_output['loss'], trainable_variables)\n",
    "  optimizer.apply(grads, trainable_variables)\n",
    "\n",
    "  return model_output\n",
    "\n",
    "train_losses = []\n",
    "train_recon_errors = []\n",
    "train_perplexities = []\n",
    "train_vqvae_loss = []\n",
    "\n",
    "for step_index, data in enumerate(train_dataset):\n",
    "  train_results = train_step(data)\n",
    "  train_losses.append(train_results['loss'])\n",
    "  train_recon_errors.append(train_results['recon_error'])\n",
    "  train_perplexities.append(train_results['vq_output']['perplexity'])\n",
    "  train_vqvae_loss.append(train_results['vq_output']['loss'])\n",
    "\n",
    "  if (step_index + 1) % 100 == 0:\n",
    "    print('%d. train loss: %f ' % (step_index + 1,\n",
    "                                   np.mean(train_losses[-100:])) +\n",
    "          ('recon_error: %.3f ' % np.mean(train_recon_errors[-100:])) +\n",
    "          ('perplexity: %.3f ' % np.mean(train_perplexities[-100:])) +\n",
    "          ('vqvae loss: %.3f' % np.mean(train_vqvae_loss[-100:])))\n",
    "  if step_index == num_training_updates:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m2hNyAnhs-1f"
   },
   "source": [
    "# Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2vo-lDyRomKD"
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(16,8))\n",
    "ax = f.add_subplot(1,2,1)\n",
    "ax.plot(train_recon_errors)\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('NMSE.')\n",
    "\n",
    "ax = f.add_subplot(1,2,2)\n",
    "ax.plot(train_perplexities)\n",
    "ax.set_title('Average codebook usage (perplexity).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lyj1CCKptCZz"
   },
   "source": [
    "# View reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rM9zj7ZiPZBG"
   },
   "outputs": [],
   "source": [
    "# Reconstructions\n",
    "train_batch = next(iter(train_dataset))\n",
    "valid_batch = next(iter(valid_dataset))\n",
    "\n",
    "# Put data through the model with is_training=False, so that in the case of \n",
    "# using EMA the codebook is not updated.\n",
    "train_reconstructions = model(train_batch['images'],\n",
    "                              is_training=False)['x_recon'].numpy()\n",
    "valid_reconstructions = model(valid_batch['images'],\n",
    "                              is_training=False)['x_recon'].numpy()\n",
    "\n",
    "\n",
    "def convert_batch_to_image_grid(image_batch):\n",
    "  reshaped = (image_batch.reshape(4, 8, 32, 32, 3)\n",
    "              .transpose(0, 2, 1, 3, 4)\n",
    "              .reshape(4 * 32, 8 * 32, 3))\n",
    "  return reshaped + 0.5\n",
    "\n",
    "\n",
    "\n",
    "f = plt.figure(figsize=(16,8))\n",
    "ax = f.add_subplot(2,2,1)\n",
    "ax.imshow(convert_batch_to_image_grid(train_batch['images'].numpy()),\n",
    "          interpolation='nearest')\n",
    "ax.set_title('training data originals')\n",
    "plt.axis('off')\n",
    "\n",
    "ax = f.add_subplot(2,2,2)\n",
    "ax.imshow(convert_batch_to_image_grid(train_reconstructions),\n",
    "          interpolation='nearest')\n",
    "ax.set_title('training data reconstructions')\n",
    "plt.axis('off')\n",
    "\n",
    "ax = f.add_subplot(2,2,3)\n",
    "ax.imshow(convert_batch_to_image_grid(valid_batch['images'].numpy()),\n",
    "          interpolation='nearest')\n",
    "ax.set_title('validation data originals')\n",
    "plt.axis('off')\n",
    "\n",
    "ax = f.add_subplot(2,2,4)\n",
    "ax.imshow(convert_batch_to_image_grid(valid_reconstructions),\n",
    "          interpolation='nearest')\n",
    "ax.set_title('validation data reconstructions')\n",
    "plt.axis('off')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "VQ_VAE_Sonnet_2_training_example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
